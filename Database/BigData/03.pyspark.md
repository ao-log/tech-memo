### データを眺める

https://localhost:8888 にアクセスし、JupyterLab を開きます。
アクセストークンは docker-compose logs | grep anaconda で確認します。

MongoDB にアクセスし、データを眺めます。
各レコードの内容を表形式で見るには、次のようにデータフレームにします。

```python
import pymongo
import pandas as pd

mongo = pymongo.MongoClient(host='mongo')
df = pd.DataFrame(list(mongo.twitter.sample.find(limit=5)))
```

スキーマは以下のようになっています。

```python
>>> df.columns
Index(['_id', '_timestamp', 'contributors', 'coordinates', 'created_at',
       'delete', 'display_text_range', 'entities', 'extended_entities',
       'favorite_count', 'favorited', 'filter_level', 'geo', 'id', 'id_str',
       'in_reply_to_screen_name', 'in_reply_to_status_id',
       'in_reply_to_status_id_str', 'in_reply_to_user_id',
       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',
       'possibly_sensitive', 'quote_count', 'reply_count', 'retweet_count',
       'retweeted', 'retweeted_status', 'source', 'text', 'timestamp_ms',
       'truncated', 'user'],
      dtype='object')
```

日本語のツイートを取り出すには、次のようにします。
ポイントは、tweets 関数が、出力するカラムをジェネレータで返していること、引数にクエリを渡せるようにしていることです。

```python
def tweets(*args, **kwargs):
    for tweet in mongo.twitter.sample.find(*args, **kwargs):
        if 'delete' in tweet:
            pass
        yield {
            'created_at': tweet['created_at'],
            'text': tweet['text']
        }

pd.DataFrame(tweets({'lang': 'ja'}, limit=10))
```

### pyspark の起動

```
$ pyspark --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.0
```

```python
# MongoDB からデータフレームに読み込み
>>> df = (spark.read
  .format("com.mongodb.spark.sql.DefaultSource")
  .option("uri","mongodb://mongo/twitter.sample")
  .load())

# ビュー「tweets」を作成
>>> df.createOrReplaceTempView('tweets')

# ビュー tweets から、言語ごとにカウント
>>> query = '''
    SELECT lang, count(*) count
    FROM tweets WHERE delete IS NULL GROUP BY 1 ORDER BY 2 DESC
'''

>>> spark.sql(query).show(3)
+----+--------+                                                                 
|lang|count(1)|
+----+--------+
|  ja|    3140|
|  en|    2984|
|  ko|     867|
+----+--------+
only showing top 3 rows
```

英語のツイートから時刻、テキストのカラムのみを抽出。

```python
>>> query='''
    SELECT from_unixtime(timestamp_ms / 1000) time, text
    FROM tweets WHERE lang = 'en'
'''

>>> en_tweets = spark.sql(query)
```

テキストを単語に分解する流れ。

```python
>>> from pyspark.sql import Row

>>> def text_split(row):
      for word in row.text.split():
         yield Row(time=row.time, word=word)

>>> en_tweets.rdd.take(1)
[Row(time='2018-06-01 12:32:56', text='xxxxxxxx')]

>>> en_tweets.rdd.flatMap(text_split).take(2)
[Row(time='2018-06-01 12:32:56', word='RT'), Row(time='2018-06-01 12:32:56', word='@xxxxxxxx:')]

>>> en_tweets.rdd.flatMap(text_split).toDF().show(2)
+-------------------+-----------------+
|               time|             word|
+-------------------+-----------------+
|2018-06-01 12:32:56|               RT|
|2018-06-01 12:32:56|@xxxxxxxx:xxxxxxx|
+-------------------+-----------------+
only showing top 2 rows
```

ワードごとに分解し、ワードのカウント。

```python
# text をワードに分解し、データフレーム「words」を作成。
>>> words = en_tweets.rdd.flatMap(text_split).toDF()

# ビュー「words」を作成
>>> words.createOrReplaceTempView('words')

>>> query = '''
... SELECT word, count(*) count
... FROM words GROUP BY 1 ORDER BY 2 DESC
... '''

# ワードごとのカウント
>>> spark.sql(query).show(3)
+----+-----+                                                                    
|word|count|
+----+-----+
|  RT| 2687|
| the| 1467|
|  to| 1177|
+----+-----+
only showing top 3 rows

# ビュー「words」を保存。「spark-warehouse」ディレクトリ下にファイルが保存される。
>>> words.write.saveAsTable('twitter_sample_words')

>>> spark.table('twitter_sample_words').count()
68193
```
